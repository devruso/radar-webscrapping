# Guia de Instala√ß√£o e Configura√ß√£o

Este guia fornece instru√ß√µes detalhadas para configurar o ambiente de desenvolvimento do projeto Radar Web Scraping.

## üìã Pr√©-requisitos

### Sistema Operacional
- Windows 10/11
- macOS 10.15+
- Linux (Ubuntu 18.04+, Debian 10+, CentOS 7+)

### Software Necess√°rio
- **Python 3.11+** - [Download](https://www.python.org/downloads/)
- **Git** - [Download](https://git-scm.com/downloads)
- **Editor de c√≥digo** (recomendado: VS Code)

### Verificar Instala√ß√µes
```bash
# Verificar Python
python --version
# Deve mostrar: Python 3.11.x ou superior

# Verificar pip
pip --version

# Verificar Git
git --version
```

## üöÄ Instala√ß√£o R√°pida

### Op√ß√£o 1: Script Autom√°tico (Recomendado)

```bash
# 1. Clone o reposit√≥rio
git clone <repository-url>
cd radar-webscrapping

# 2. Execute o setup autom√°tico
python setup.py
```

O script autom√°tico ir√°:
- ‚úÖ Atualizar pip para vers√£o mais recente
- ‚úÖ Instalar todas as depend√™ncias Python
- ‚úÖ Configurar navegadores Playwright
- ‚úÖ Instalar depend√™ncias do sistema
- ‚úÖ Verificar instala√ß√£o

### Op√ß√£o 2: Instala√ß√£o Manual

Se preferir controle total sobre o processo:

```bash
# 1. Clone o reposit√≥rio
git clone <repository-url>
cd radar-webscrapping

# 2. Criar ambiente virtual (recomendado)
python -m venv venv

# 3. Ativar ambiente virtual
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# 4. Atualizar pip
python -m pip install --upgrade pip

# 5. Instalar depend√™ncias
pip install -r requirements.txt

# 6. Instalar navegadores Playwright
playwright install

# 7. Instalar depend√™ncias do sistema (opcional)
playwright install-deps
```

## ‚öôÔ∏è Configura√ß√£o

### 1. Arquivo de Ambiente

Copie o arquivo de exemplo e configure suas vari√°veis:

```bash
# Copiar arquivo de exemplo
cp .env.example .env

# Editar configura√ß√µes
# Windows: notepad .env
# macOS/Linux: nano .env
```

### 2. Configura√ß√µes Principais

Edite o arquivo `.env` com suas configura√ß√µes:

```env
# ========================================
# CONFIGURA√á√ïES DA API BACKEND
# ========================================
API_BASE_URL=http://localhost:8080/api
API_TIMEOUT=30
API_MAX_RETRIES=3

# ========================================
# CONFIGURA√á√ïES DO BROWSER
# ========================================
# true = browser invis√≠vel (produ√ß√£o)
# false = browser vis√≠vel (debug)
BROWSER_HEADLESS=true

# Delay entre requisi√ß√µes (em segundos)
BROWSER_RATE_LIMIT=1.0

# User agent para requisi√ß√µes
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# ========================================
# CONFIGURA√á√ïES DE LOGGING
# ========================================
LOG_LEVEL=INFO
LOG_FILE=logs/scraper.log

# ========================================
# CONFIGURA√á√ïES DE SCRAPING
# ========================================
# M√°ximo de scrapers executando simultaneamente
MAX_CONCURRENT_SCRAPERS=2

# Timeout para scraping (em segundos)
SCRAPING_TIMEOUT=300

# Tentativas de retry em caso de erro
RETRY_ATTEMPTS=3

# ========================================
# URLs DOS SITES UNIVERSIT√ÅRIOS
# ========================================
# Configure conforme sua universidade
SITE_COURSES_URL=https://example-university.edu/courses
SITE_SCHEDULES_URL=https://example-university.edu/schedules
SITE_SYLLABI_URL=https://example-university.edu/syllabi

# ========================================
# CONFIGURA√á√ïES DE DESENVOLVIMENTO
# ========================================
DEBUG=true
RELOAD_ON_CHANGE=true
```

### 3. Configura√ß√µes por Universidade

#### Universidade de Bras√≠lia (UnB)
```env
SITE_COURSES_URL=https://matriculaweb.unb.br/graduacao/oferta_dis.aspx
SITE_SCHEDULES_URL=https://matriculaweb.unb.br/graduacao/consulta_oferta.aspx
SITE_SYLLABI_URL=https://matriculaweb.unb.br/graduacao/ementas.aspx
```

#### USP
```env
SITE_COURSES_URL=https://uspdigital.usp.br/jupiterweb
SITE_SCHEDULES_URL=https://uspdigital.usp.br/jupiterweb/listarGradeCurricular
SITE_SYLLABI_URL=https://uspdigital.usp.br/jupiterweb/obterDisciplina
```

#### UNICAMP
```env
SITE_COURSES_URL=https://www.dac.unicamp.br/sistemas/catalogos/
SITE_SCHEDULES_URL=https://www.dac.unicamp.br/sistemas/horarios/
SITE_SYLLABI_URL=https://www.dac.unicamp.br/sistemas/ementas/
```

## üîß Configura√ß√£o Avan√ßada

### 1. Configura√ß√µes de Proxy

Se sua institui√ß√£o usa proxy:

```env
# Adicionar ao .env
HTTP_PROXY=http://proxy.university.edu:8080
HTTPS_PROXY=http://proxy.university.edu:8080
NO_PROXY=localhost,127.0.0.1
```

### 2. Configura√ß√µes de Banco de Dados

Para armazenamento local (opcional):

```env
# PostgreSQL
DATABASE_URL=postgresql://user:password@localhost:5432/radar_scraping

# SQLite (mais simples)
DATABASE_URL=sqlite:///./radar_data.db

# MongoDB
MONGODB_URL=mongodb://localhost:27017/radar_scraping
```

### 3. Configura√ß√µes de Redis (Cache)

Para cache avan√ßado:

```env
REDIS_URL=redis://localhost:6379/0
CACHE_TTL=3600  # 1 hora
```

## üèÉ‚Äç‚ôÇÔ∏è Executar o Sistema

### 1. Iniciar API em Desenvolvimento

```bash
# M√©todo 1: Script de inicializa√ß√£o
python run.py

# M√©todo 2: Diretamente
uvicorn src.main:app --reload --host localhost --port 8000
```

### 2. Verificar se Funcionou

Abra seu navegador e acesse:
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **Status**: http://localhost:8000/

Voc√™ deve ver:
```json
{
  "name": "Radar Web Scraping API",
  "version": "1.0.0",
  "status": "running",
  "available_scrapers": ["courses", "schedules", "syllabus"]
}
```

## üß™ Testar Instala√ß√£o

### 1. Teste B√°sico via API

```bash
# Testar health check
curl http://localhost:8000/health

# Listar scrapers dispon√≠veis
curl http://localhost:8000/scrapers

# Executar scraping de teste
curl -X POST http://localhost:8000/scraping/start \
  -H "Content-Type: application/json" \
  -d '{
    "scraping_type": "courses",
    "config": {"max_pages": 1},
    "send_to_backend": false
  }'
```

### 2. Teste via Interface Web

1. Acesse http://localhost:8000/docs
2. Expanda o endpoint `POST /scraping/start`
3. Clique em "Try it out"
4. Modifique o JSON:
```json
{
  "scraping_type": "courses",
  "config": {
    "max_pages": 2
  },
  "send_to_backend": false
}
```
5. Clique "Execute"

### 3. Teste Program√°tico

Crie um arquivo `test_scraping.py`:

```python
import asyncio
from src.scrapers.course_scraper import CourseScraper

async def test_scraper():
    scraper = CourseScraper()
    
    # Configura√ß√£o de teste
    config = {
        'max_pages': 1,
        'debug': True
    }
    
    try:
        results = await scraper.scrape(config)
        print(f"‚úÖ Teste bem-sucedido! {len(results)} cursos encontrados")
        
        for course in results[:3]:  # Mostrar apenas 3
            print(f"- {course.name} ({course.code})")
            
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")

if __name__ == "__main__":
    asyncio.run(test_scraper())
```

Execute:
```bash
python test_scraping.py
```

## üõ†Ô∏è Troubleshooting

### Problemas Comuns

#### 1. "ModuleNotFoundError: No module named 'playwright'"

**Solu√ß√£o:**
```bash
# Verificar se est√° no ambiente virtual correto
pip list | grep playwright

# Se n√£o estiver instalado:
pip install playwright
playwright install
```

#### 2. "playwright._impl._api_types.Error: Browser executable doesn't exist"

**Solu√ß√£o:**
```bash
# Reinstalar navegadores
playwright install --force

# Verificar instala√ß√£o
playwright --version
```

#### 3. "TimeoutError: Timeout 30000ms exceeded"

**Causas poss√≠veis:**
- Conex√£o lenta com internet
- Site universit√°rio est√° lento/fora do ar
- Configura√ß√µes de proxy

**Solu√ß√µes:**
```env
# Aumentar timeout no .env
SCRAPING_TIMEOUT=600

# Ou desabilitar headless para debug
BROWSER_HEADLESS=false
```

#### 4. "Permission denied" no Linux/macOS

**Solu√ß√£o:**
```bash
# Dar permiss√µes de execu√ß√£o
chmod +x setup.py
chmod +x run.py

# Ou executar com python diretamente
python setup.py
python run.py
```

#### 5. Erro de SSL/Certificados

**Solu√ß√£o:**
```env
# Adicionar ao .env para desenvolvimento
PYTHONHTTPSVERIFY=0
```

Ou instalar certificados:
```bash
# macOS
/Applications/Python\ 3.11/Install\ Certificates.command

# Linux
sudo apt-get update && sudo apt-get install ca-certificates
```

### Logs e Debugging

#### 1. Verificar Logs

```bash
# Ver logs da aplica√ß√£o
tail -f logs/scraper.log

# Ver logs do sistema
# Windows: Event Viewer
# Linux: journalctl -f
# macOS: Console.app
```

#### 2. Executar em Modo Debug

```bash
# Definir n√≠vel de log
export LOG_LEVEL=DEBUG

# Executar com browser vis√≠vel
export BROWSER_HEADLESS=false

python run.py
```

#### 3. Testar Conectividade

```python
# test_connectivity.py
import asyncio
import aiohttp

async def test_urls():
    urls = [
        "https://example-university.edu/courses",
        "http://localhost:8080/api/health"
    ]
    
    async with aiohttp.ClientSession() as session:
        for url in urls:
            try:
                async with session.get(url, timeout=10) as response:
                    print(f"‚úÖ {url}: {response.status}")
            except Exception as e:
                print(f"‚ùå {url}: {e}")

asyncio.run(test_urls())
```

### Performance

#### 1. Otimizar para M√°quina Local

```env
# Para m√°quinas com pouca RAM
MAX_CONCURRENT_SCRAPERS=1
BROWSER_HEADLESS=true

# Para m√°quinas potentes
MAX_CONCURRENT_SCRAPERS=4
```

#### 2. Monitorar Recursos

```bash
# Ver uso de CPU/mem√≥ria
top
# ou
htop

# Ver processos Python
ps aux | grep python
```

## üê≥ Docker (Opcional)

Para um ambiente completamente isolado:

### 1. Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar depend√™ncias do sistema
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Copiar requirements primeiro (cache layer)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Instalar Playwright
RUN playwright install chromium
RUN playwright install-deps chromium

# Copiar c√≥digo
COPY . .

# Expor porta
EXPOSE 8000

# Comando de inicializa√ß√£o
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. docker-compose.yml

```yml
version: '3.8'

services:
  scraper:
    build: .
    ports:
      - "8000:8000"
    environment:
      - BROWSER_HEADLESS=true
      - API_BASE_URL=http://backend:8080/api
    volumes:
      - ./logs:/app/logs
      - ./.env:/app/.env
    depends_on:
      - backend

  backend:
    image: your-backend-image
    ports:
      - "8080:8080"
```

### 3. Executar com Docker

```bash
# Build e executar
docker-compose up --build

# Apenas executar
docker-compose up

# Em background
docker-compose up -d
```

---

## ‚úÖ Checklist Final

Antes de come√ßar a usar o sistema, verifique:

- [ ] Python 3.11+ instalado e funcionando
- [ ] Depend√™ncias instaladas (`pip list` mostra todas)
- [ ] Navegadores Playwright configurados
- [ ] Arquivo `.env` configurado com suas URLs
- [ ] API iniciando sem erros (`python run.py`)
- [ ] Health check retornando sucesso (`curl localhost:8000/health`)
- [ ] Teste b√°sico de scraping funcionando

## üìû Suporte

Se encontrar problemas:

1. **Verifique os logs**: `logs/scraper.log`
2. **Consulte troubleshooting** acima
3. **Teste conectividade** com sites universit√°rios
4. **Abra issue** no reposit√≥rio com:
   - Sistema operacional
   - Vers√£o do Python
   - Logs de erro completos
   - Passos para reproduzir

---

**Configura√ß√£o conclu√≠da! üéâ Agora voc√™ pode come√ßar a usar o sistema de scraping.**