# ğŸ¯ Radar WebScrapping

Sistema de coleta automatizada de dados acadÃªmicos do SIGAA UFBA, desenvolvido seguindo **Clean Architecture** e princÃ­pios **SOLID**.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Architecture](https://img.shields.io/badge/Architecture-Clean-green.svg)](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
[![SOLID](https://img.shields.io/badge/Principles-SOLID-orange.svg)](https://en.wikipedia.org/wiki/SOLID)
[![Docker](https://img.shields.io/badge/Docker-Enabled-2496ED.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Sistema robusto e escalÃ¡vel para coleta de dados acadÃªmicos do SIGAA UFBA, fornecendo informaÃ§Ãµes estruturadas sobre cursos, componentes curriculares e estruturas curriculares para o sistema Radar de recomendaÃ§Ã£o acadÃªmica.

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura Clean](#arquitetura-clean)
3. [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
4. [Docker](#docker)
5. [Guia de Uso](#guia-de-uso)
6. [Desenvolvimento](#desenvolvimento)
7. [IntegraÃ§Ã£o Radar-Infra](#integraÃ§Ã£o-radar-infra)
8. [Troubleshooting](#troubleshooting)

## ğŸ¯ VisÃ£o Geral

O **Radar WebScrapping** Ã© o componente de coleta de dados do Sistema Radar de RecomendaÃ§Ã£o AcadÃªmica da UFBA. Desenvolvido seguindo **Clean Architecture** e princÃ­pios **SOLID**, oferece:

### ğŸ”§ Funcionalidades Principais

- **ğŸ“š Scraping de Cursos**: Coleta dados dos cursos de graduaÃ§Ã£o do SIGAA UFBA
- **ğŸ“‹ Componentes Curriculares**: ExtraÃ§Ã£o de disciplinas e atividades acadÃªmicas  
- **ğŸ—ï¸ Estruturas Curriculares**: Mapeamento de grades curriculares e prÃ©-requisitos
- **ğŸ”„ SincronizaÃ§Ã£o**: IntegraÃ§Ã£o automÃ¡tica com radar-webapi (Spring Boot)
- **ğŸ’» Interface CLI**: Controle completo via linha de comando
- **ğŸ³ Docker Ready**: ContainerizaÃ§Ã£o para deploy simplificado

### âœ¨ CaracterÃ­sticas TÃ©cnicas

âœ… **Clean Architecture**: SeparaÃ§Ã£o clara de responsabilidades em camadas  
âœ… **SOLID Principles**: CÃ³digo maintÃ­vel e extensÃ­vel  
âœ… **Async/Await**: OperaÃ§Ãµes assÃ­ncronas para melhor performance  
âœ… **SQLAlchemy ORM**: PersistÃªncia de dados robusta  
âœ… **Selenium WebDriver**: AutomaÃ§Ã£o web confiÃ¡vel  
âœ… **Structured Logging**: Observabilidade com Loguru  
âœ… **Type Safety**: ValidaÃ§Ã£o com Pydantic  

## ğŸ—ï¸ Arquitetura Clean

O sistema segue rigorosamente os princÃ­pios de **Clean Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 INTERFACES                          â”‚
â”‚    CLI Controllers | REST API | Web Interface      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               APPLICATION                           â”‚
â”‚   Use Cases | DTOs | Repository Interfaces         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DOMAIN                             â”‚
â”‚     Entities | Value Objects | Domain Logic        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INFRASTRUCTURE                         â”‚
â”‚   Scrapers | Repositories | External APIs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‚ Estrutura de DiretÃ³rios

```
src/
â”œâ”€â”€ domain/                    # ğŸ”µ Camada de DomÃ­nio
â”‚   â”œâ”€â”€ entities/             # Entidades de negÃ³cio
â”‚   â”œâ”€â”€ value_objects/        # Objetos de valor
â”‚   â””â”€â”€ exceptions/           # ExceÃ§Ãµes de domÃ­nio
â”œâ”€â”€ application/              # ğŸŸ¡ Camada de AplicaÃ§Ã£o  
â”‚   â”œâ”€â”€ use_cases/            # Casos de uso
â”‚   â”œâ”€â”€ interfaces/           # Contratos/AbstraÃ§Ãµes
â”‚   â””â”€â”€ dtos/                 # Data Transfer Objects
â”œâ”€â”€ infrastructure/           # ğŸ”´ Camada de Infraestrutura
â”‚   â”œâ”€â”€ scrapers/            # ImplementaÃ§Ãµes de scraping
â”‚   â”œâ”€â”€ repositories/        # PersistÃªncia de dados
â”‚   â””â”€â”€ api_clients/         # Clientes HTTP
â”œâ”€â”€ interfaces/              # ğŸŸ¢ Camada de Interface
â”‚   â””â”€â”€ cli/                 # Controllers CLI
â””â”€â”€ shared/                  # ğŸŸ  UtilitÃ¡rios
    â””â”€â”€ logging.py           # ConfiguraÃ§Ã£o de logs
```

```python
scraper = scraper_registry.create_scraper(ScrapingType.COURSES)
```

#### 3. Observer Pattern
Monitoramento de progresso:

```python
class ScrapingJob:
    def notify_progress(self, progress):
        for observer in self.observers:
            observer.update(progress)
```

### TÃ©cnicas Anti-DetecÃ§Ã£o

#### 1. User Agent Rotation
```python
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
]
```

#### 2. Rate Limiting
```python
await asyncio.sleep(random.uniform(1, 3))  # Delay aleatÃ³rio
```

#### 3. Headers Customizados
```python
headers = {
    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1'
}
```

### Processamento de PDFs

#### Bibliotecas Utilizadas

1. **pdfplumber**: Melhor para textos estruturados
2. **PyMuPDF**: Melhor para layouts complexos

```python
# EstratÃ©gia dupla
pdfplumber_text = extract_with_pdfplumber(pdf_path)
pymupdf_text = extract_with_pymupdf(pdf_path)

# Escolher melhor resultado baseado em score de confianÃ§a
best_text = choose_best_extraction(pdfplumber_text, pymupdf_text)
```

## ğŸ—ï¸ Arquitetura do Sistema

### Estrutura de DiretÃ³rios

```
radar-webscrapping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # Modelos de dados Pydantic
â”‚   â”‚   â””â”€â”€ scraped_data.py
â”‚   â”œâ”€â”€ scrapers/         # Scrapers especializados
â”‚   â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”‚   â”œâ”€â”€ course_scraper.py
â”‚   â”‚   â”œâ”€â”€ schedule_scraper.py
â”‚   â”‚   â””â”€â”€ syllabus_scraper.py
â”‚   â”œâ”€â”€ services/         # ServiÃ§os de infraestrutura
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ pdf_processor.py
â”‚   â”œâ”€â”€ utils/            # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ browser_manager.py
â”‚   â”‚   â””â”€â”€ data_validator.py
â”‚   â”œâ”€â”€ config/           # ConfiguraÃ§Ãµes
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â””â”€â”€ main.py           # API FastAPI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py             # Script de configuraÃ§Ã£o
â”œâ”€â”€ run.py              # Inicializador
â””â”€â”€ README.md
```

### Fluxo de Dados

1. **ConfiguraÃ§Ã£o**: Carregamento de settings e inicializaÃ§Ã£o
2. **Scraping**: Coleta de dados dos sites universitÃ¡rios
3. **Processamento**: Limpeza e estruturaÃ§Ã£o dos dados
4. **ValidaÃ§Ã£o**: VerificaÃ§Ã£o de qualidade e completude
5. **SincronizaÃ§Ã£o**: Envio para backend via API REST

### Componentes Principais

#### 1. Browser Manager
Gerencia instÃ¢ncias do Playwright com configuraÃ§Ãµes anti-detecÃ§Ã£o:

```python
browser_manager = BrowserManager()
page = await browser_manager.get_page()
```

#### 2. Scraper Registry
Factory para criaÃ§Ã£o e gerenciamento de scrapers:

```python
scraper_registry.register(CourseScraper)
scraper = scraper_registry.create_scraper(ScrapingType.COURSES)
```

#### 3. API Client
Cliente HTTP para comunicaÃ§Ã£o com backend:

```python
await api_client.send_courses(courses_data)
await api_client.send_schedules(schedules_data)
```

#### 4. PDF Processor
Processamento inteligente de documentos PDF:

```python
content = await pdf_processor.extract_syllabus_content(pdf_path)
```

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Python 3.11+
- pip
- ConexÃ£o com internet

### InstalaÃ§Ã£o AutomÃ¡tica

```bash
# 1. Clone o repositÃ³rio
git clone <repository-url>
cd radar-webscrapping

# 2. Execute o setup automÃ¡tico
python setup.py
```

O script `setup.py` irÃ¡:
- Instalar dependÃªncias Python
- Configurar navegadores Playwright
- Verificar configuraÃ§Ã£o

### InstalaÃ§Ã£o Manual

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Instalar navegadores Playwright
playwright install
playwright install-deps

# 3. Configurar ambiente
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

### ConfiguraÃ§Ã£o Local

Copie `.env.example` para `.env` e configure:

```env
# ConfiguraÃ§Ãµes do SIGAA UFBA
SIGAA_BASE_URL=https://sigaa.ufba.br
SIGAA_DELAY_BETWEEN_REQUESTS=2.0

# Banco de dados local
DATABASE_URL=sqlite+aiosqlite:///data/radar_webscrapping.db

# API do Radar (integraÃ§Ã£o)
RADAR_API_BASE_URL=http://localhost:8080/api
RADAR_API_TIMEOUT=30

# Selenium
SELENIUM_HEADLESS=true
SELENIUM_TIMEOUT=30
```

## ğŸ³ Docker

O sistema estÃ¡ totalmente preparado para execuÃ§Ã£o em Docker, com suporte completo ao **radar-infra**.

### ğŸ”§ Desenvolvimento Local com Docker

```bash
# Clonar o repositÃ³rio
git clone <repository-url>
cd radar-webscrapping

# Construir e executar com docker-compose
docker-compose up --build

# ServiÃ§os disponÃ­veis:
# - App: http://localhost:8000
# - PostgreSQL: localhost:5432  
# - Redis: localhost:6379
# - PgAdmin: http://localhost:5050 (desenvolvimento)
# - Jupyter: http://localhost:8888 (desenvolvimento)
```

### ğŸ—ï¸ IntegraÃ§Ã£o com radar-infra

Para execuÃ§Ã£o no ambiente completo do sistema Radar:

1. **Clone o radar-infra**:
```bash
git clone <radar-infra-repository>
cd radar-infra
```

2. **Configure o .env do radar-infra**:
```env
# VersÃµes
WEBSCRAPPING_VERSION=latest

# ConfiguraÃ§Ãµes especÃ­ficas
RADAR_API_KEY=your-production-api-key
SCRAPING_SCHEDULE=0 2 * * *  # 2h da manhÃ£
SCRAPING_LOG_LEVEL=INFO
```

3. **Execute o sistema completo**:
```bash
docker-compose up -d
```

### ğŸ“¦ Comandos Docker DisponÃ­veis

```bash
# Executar CLI
docker run --rm -it radar/webscrapping:latest cli --help

# Scraping de cursos especÃ­fico
docker run --rm radar/webscrapping:latest scrape-cursos --unidade "IME"

# Executar com todas as dependÃªncias
docker-compose run --rm webscrapping cli scrape-cursos

# Acessar container para debug
docker-compose exec webscrapping bash

# Ver logs em tempo real
docker-compose logs -f webscrapping
```

### ğŸ¯ Modos de ExecuÃ§Ã£o

O container suporta diferentes modos atravÃ©s do **entrypoint**:

- **`api`**: Inicia servidor FastAPI (padrÃ£o)
- **`cli`**: Interface linha de comando
- **`scrape-cursos`**: Executa scraping de cursos
- **`scrape-componentes`**: Executa scraping de componentes
- **`scrape-estruturas`**: Executa scraping de estruturas
- **`scheduler`**: Inicia agendador automÃ¡tico
- **`worker`**: Inicia worker para processamento background

### ğŸ” Health Check

O container inclui health check automÃ¡tico:

```bash
# Verificar status do container
docker ps

# Verificar logs do health check
docker inspect --format='{{json .State.Health}}' radar-webscrapping
```

## ğŸ“– Guia de Uso

### Iniciar o Sistema

```bash
# Executar API em modo desenvolvimento
python run.py

# API estarÃ¡ disponÃ­vel em http://localhost:8000
```

### Uso via API REST

#### 1. Verificar Status do Sistema

```bash
curl http://localhost:8000/health
```

#### 2. Listar Scrapers DisponÃ­veis

```bash
curl http://localhost:8000/scrapers
```

#### 3. Executar Scraping Individual

```bash
curl -X POST http://localhost:8000/scraping/start \
  -H "Content-Type: application/json" \
  -d '{
    "scraping_type": "courses",
    "config": {"max_pages": 5},
    "send_to_backend": true
  }'
```

#### 4. Executar Scraping em Lote

```bash
curl -X POST http://localhost:8000/scraping/batch \
  -H "Content-Type: application/json" \
  -d '{
    "scraping_types": ["courses", "schedules", "syllabus"],
    "config": {"max_pages": 10},
    "max_concurrent": 2
  }'
```

#### 5. Monitorar Jobs

```bash
# Listar jobs
curl http://localhost:8000/scraping/jobs

# Status especÃ­fico
curl http://localhost:8000/scraping/jobs/{job_id}

# Resultados
curl http://localhost:8000/scraping/results/{job_id}
```

### Uso ProgramÃ¡tico

```python
from src.scrapers.course_scraper import CourseScraper
from src.services.api_client import api_client

# Criar scraper
scraper = CourseScraper()

# Executar scraping
results = await scraper.scrape({
    'max_pages': 5,
    'search_terms': ['computaÃ§Ã£o', 'engenharia']
})

# Enviar para backend
await api_client.send_courses(results)
```

## ğŸ‘¨â€ğŸ’» Desenvolvimento

### Criando um Novo Scraper

#### 1. Definir Modelo de Dados

```python
# src/models/scraped_data.py
class NewData(BaseScrapedData):
    field1: str
    field2: Optional[int] = None
    field3: List[str] = Field(default_factory=list)
```

#### 2. Implementar Scraper

```python
# src/scrapers/new_scraper.py
from .base_scraper import BaseScraper
from ..models.scraped_data import NewData, ScrapingType

class NewScraper(BaseScraper):
    def __init__(self):
        super().__init__(ScrapingType.NEW_TYPE)
    
    async def scrape(self, config: Dict[str, Any] = None) -> List[NewData]:
        # Implementar lÃ³gica de scraping
        results = []
        
        page = await browser_manager.get_page()
        await page.goto(self.base_url)
        
        # Extrair dados
        elements = await page.query_selector_all('.data-element')
        
        for element in elements:
            data = NewData(
                field1=await element.inner_text(),
                field2=42,
                field3=['item1', 'item2']
            )
            results.append(data)
        
        return results
```

#### 3. Registrar Scraper

```python
# src/main.py
from .scrapers.new_scraper import NewScraper

# No lifespan da aplicaÃ§Ã£o
scraper_registry.register(NewScraper)
```

### Melhores PrÃ¡ticas

#### 1. Tratamento de Erros

```python
try:
    await page.goto(url, timeout=30000)
except PlaywrightTimeoutError:
    logger.error(f"Timeout ao carregar {url}")
    return []
except Exception as e:
    logger.error(f"Erro inesperado: {e}")
    raise
```

#### 2. Rate Limiting

```python
async def _apply_rate_limit(self):
    await asyncio.sleep(self.rate_limit + random.uniform(0, 1))
```

#### 3. ValidaÃ§Ã£o de Dados

```python
# Usar Pydantic para validaÃ§Ã£o automÃ¡tica
class Course(BaseModel):
    name: str = Field(..., min_length=1, max_length=200)
    code: str = Field(..., regex=r'^[A-Z]{2,4}\d{3,4}$')
    credits: int = Field(..., ge=1, le=20)
```

#### 4. Logging Estruturado

```python
from loguru import logger

logger.info("Iniciando scraping", extra={
    "scraper": self.__class__.__name__,
    "url": url,
    "timestamp": datetime.now().isoformat()
})
```

### Debugging e Testes

#### 1. Modo Debug

```python
# Executar com browser visÃ­vel
browser_manager = BrowserManager(headless=False)
```

#### 2. Screenshots para Debug

```python
await page.screenshot(path="debug_screenshot.png")
```

#### 3. Interceptar Requests

```python
async def intercept_request(request):
    logger.info(f"Request: {request.url}")

page.on("request", intercept_request)
```

## ğŸ“š API Reference

### Endpoints Principais

#### GET /
InformaÃ§Ãµes gerais da API

#### GET /health
Health check dos componentes

#### GET /scrapers
Lista scrapers disponÃ­veis

#### POST /scraping/start
Inicia job individual
- `scraping_type`: Tipo do scraper
- `config`: ConfiguraÃ§Ãµes especÃ­ficas
- `send_to_backend`: Auto-sincronizaÃ§Ã£o

#### POST /scraping/batch
Inicia mÃºltiplos jobs
- `scraping_types`: Lista de tipos
- `max_concurrent`: Limite de concorrÃªncia

#### GET /scraping/jobs
Lista jobs ativos
- `status`: Filtro por status
- `limit`: Limite de resultados

#### GET /scraping/jobs/{job_id}
Detalhes de job especÃ­fico

#### GET /stats
EstatÃ­sticas do sistema

### Modelos de Dados

#### ScrapingJob
```python
{
    "job_id": "uuid",
    "scraping_type": "courses|schedules|syllabus",
    "status": "pending|running|completed|failed",
    "created_at": "2024-01-15T10:30:00",
    "config": {}
}
```

#### Course
```python
{
    "name": "Algoritmos e Estruturas de Dados",
    "code": "CIC0123",
    "credits": 4,
    "department": "CiÃªncia da ComputaÃ§Ã£o",
    "prerequisites": ["CIC0456"],
    "description": "Estudo de algoritmos...",
    "source_url": "https://..."
}
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

#### 1. "Browser nÃ£o iniciou"
```bash
# Reinstalar navegadores
playwright install --force
```

#### 2. "TimeoutError ao carregar pÃ¡gina"
- Verificar conexÃ£o internet
- Aumentar timeout nas configuraÃ§Ãµes
- Site pode estar bloqueando requests

#### 3. "Dados nÃ£o encontrados"
- Site pode ter mudado estrutura
- Verificar seletores CSS/XPath
- Executar em modo debug (headless=false)

#### 4. "Erro de importaÃ§Ã£o"
```bash
# Verificar instalaÃ§Ã£o das dependÃªncias
pip install -r requirements.txt
```

### Logs e Debugging

#### Habilitar Logs Detalhados
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Ver Logs da API
```bash
# Logs aparecem no terminal onde executou python run.py
```

#### Screenshots de Debug
```python
# Adicionar no scraper
await page.screenshot(path=f"debug_{datetime.now()}.png")
```

### Performance

#### Otimizar Scraping
- Reduzir `rate_limit` se o site permitir
- Usar `max_concurrent` adequado (2-3 Ã© bom)
- Filtrar dados desnecessÃ¡rios no CSS selector

#### Monitorar Recursos
```bash
# Verificar uso de memÃ³ria
curl http://localhost:8000/stats
```

## ğŸ”— IntegraÃ§Ã£o Radar-Infra

O **radar-webscrapping** Ã© um dos 4 componentes do Sistema Radar e integra-se perfeitamente com o **radar-infra**:

### ğŸ—ï¸ Arquitetura do Sistema Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  radar-webapp   â”‚    â”‚ radar-webscrapping â”‚    â”‚  radar-webapi   â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚    (Python)        â”‚â—„â”€â”€â–ºâ”‚  (Spring Boot)  â”‚
â”‚   Port: 3000    â”‚    â”‚    Port: 8000      â”‚    â”‚   Port: 8080    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     MySQL       â”‚
                         â”‚   Port: 3306    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ ConfiguraÃ§Ã£o no radar-infra

O **docker-compose.yml** do radar-infra jÃ¡ inclui a configuraÃ§Ã£o completa:

```yaml
radar-webscrapping:
  build: 
    context: ./radar-webscrapping
    dockerfile: Dockerfile
  image: radar/webscrapping:${WEBSCRAPPING_VERSION:-latest}
  container_name: radar-webscrapping
  restart: unless-stopped
  ports:
    - "${WEBSCRAPPING_PORT:-8000}:8000"
  environment:
    RADAR_API_URL: http://radar-webapi:8080
    RADAR_API_KEY: ${RADAR_API_KEY}
    SCRAPING_SCHEDULE: ${SCRAPING_SCHEDULE:-0 2 * * *}
    LOG_LEVEL: ${SCRAPING_LOG_LEVEL:-INFO}
  depends_on:
    radar-webapi:
      condition: service_healthy
  networks:
    - radar-network
  volumes:
    - scraping_data:/app/data
    - scraping_logs:/app/logs
```

### ğŸ”§ VariÃ¡veis de Ambiente (radar-infra)

Configure no `.env` do **radar-infra**:

```env
# VersÃ£o do WebScrapping  
WEBSCRAPPING_VERSION=latest
WEBSCRAPPING_PORT=8000

# ConfiguraÃ§Ãµes de integraÃ§Ã£o
RADAR_API_KEY=your-secure-api-key-here
SCRAPING_SCHEDULE=0 2 * * *  # Todo dia Ã s 2h da manhÃ£
SCRAPING_LOG_LEVEL=INFO

# URL da API para frontend
VITE_WEBSCRAPPING_URL=http://localhost:8000
```

### ğŸš€ Deploy Completo

```bash
# 1. Clone o radar-infra
git clone <radar-infra-repository>
cd radar-infra

# 2. Clone todos os sub-projetos
git clone <radar-webscrapping-repository>
git clone <radar-webapi-repository>  
git clone <radar-webapp-repository>

# 3. Configure .env
cp .env.example .env
# Edite conforme necessÃ¡rio

# 4. Execute todo o sistema
docker-compose up -d

# 5. Verifique status
docker-compose ps
```

### ğŸ“Š Monitoramento Integrado

```bash
# Logs de todos os serviÃ§os
docker-compose logs -f

# Logs apenas do webscrapping
docker-compose logs -f radar-webscrapping

# Health check do sistema
curl http://localhost:8080/actuator/health  # webapi
curl http://localhost:8000/health           # webscrapping
curl http://localhost:3000                  # webapp
```

### ğŸ”„ Fluxo de Dados Integrado

1. **WebScrapping** coleta dados do SIGAA UFBA
2. **WebScrapping** envia dados para **WebAPI** via REST
3. **WebAPI** armazena no MySQL e processa
4. **WebApp** consome dados via **WebAPI**
5. **UsuÃ¡rios** acessam via **WebApp**

---

## ğŸ“ Desenvolvimento e ContribuiÃ§Ã£o

### ğŸ› ï¸ Setup para Desenvolvimento

```bash
# 1. Fork e clone
git clone <your-fork>
cd radar-webscrapping

# 2. Instalar dependÃªncias  
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 3. Pre-commit hooks
pre-commit install

# 4. Executar testes
pytest tests/

# 5. Verificar cobertura
pytest --cov=src tests/
```

### ğŸ“š Recursos de Aprendizado

- **Clean Architecture**: [Uncle Bob's Blog](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)
- **SOLID Principles**: [SOLID Principles Explained](https://en.wikipedia.org/wiki/SOLID)
- **Selenium Documentation**: [selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/)
- **SQLAlchemy**: [docs.sqlalchemy.org](https://docs.sqlalchemy.org/)
- **FastAPI**: [fastapi.tiangolo.com](https://fastapi.tiangolo.com/)

### ğŸ¤ Como Contribuir

1. **Issues**: Reporte bugs ou sugira melhorias
2. **Pull Requests**: Implemente funcionalidades seguindo Clean Architecture
3. **DocumentaÃ§Ã£o**: Melhore docs e exemplos
4. **Testes**: Adicione cobertura de testes
5. **Code Review**: Participe das revisÃµes

---

**ğŸ¯ Sistema Radar - RecomendaÃ§Ã£o AcadÃªmica UFBA**  
**Desenvolvido seguindo Clean Architecture e PrincÃ­pios SOLID**  
**VersÃ£o 1.0 - 2024**